{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQhLwgW4kALa"
      },
      "outputs": [],
      "source": [
        "#enhanced_recommender.py\n",
        "# 30.04\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark   = False\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import BertModel, BertConfig, get_cosine_schedule_with_warmup\n",
        "from captum.attr import IntegratedGradients\n",
        "import shap\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- helper functions for attention-entropy & plots ----\n",
        "def compute_layer_head_entropies(attn_weights, layer):\n",
        "    W = attn_weights[layer]            # (B, H, L, L)\n",
        "    avg = W.mean(dim=0).mean(dim=1)    # (H, L)\n",
        "    H, L = avg.shape\n",
        "    ents = []\n",
        "    for h in range(H):\n",
        "        p = avg[h] / (avg[h].sum() + 1e-12)\n",
        "        ent = -(p * torch.log(p + 1e-12)).sum().item() / math.log(L)\n",
        "        ents.append(ent)\n",
        "    hmax_idx = int(np.argmax(ents))\n",
        "    hmin_idx = int(np.argmin(ents))\n",
        "    return ents, hmax_idx, hmin_idx\n",
        "\n",
        "def plot_layer_entropy(attn_weights, layer):\n",
        "    ents, _, _ = compute_layer_head_entropies(attn_weights, layer)\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, len(ents)+1), ents, marker='o')\n",
        "    plt.xlabel(\"Head\")\n",
        "    plt.ylabel(\"Normalized Entropy\")\n",
        "    plt.title(f\"Layer {layer+1} Head Entropies\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_attention_heatmap(attn_weights, layer, head_idx, title=\"\"):\n",
        "    heatmap = attn_weights[layer][0, head_idx].detach().cpu().numpy()\n",
        "    plt.figure()\n",
        "    plt.imshow(heatmap, aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Layer {layer+1} Head {head_idx+1} Attention {title}\")\n",
        "    plt.xlabel(\"Key Position\")\n",
        "    plt.ylabel(\"Query Position\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Reproducibility Helper ===\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark   = False\n",
        "\n",
        "# === Bootstrap CI helper ===\n",
        "def bootstrap_ci(data, n_bootstrap=1000, alpha=0.05):\n",
        "    boot_means = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(data, size=len(data), replace=True)\n",
        "        boot_means.append(np.mean(sample))\n",
        "    lower = np.percentile(boot_means, 100 * alpha/2)\n",
        "    upper = np.percentile(boot_means, 100 * (1 - alpha/2))\n",
        "    return lower, upper, np.mean(data)\n",
        "\n",
        "# === Utility functions ===\n",
        "def safe_float(x, default=0.0):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def load_json_file(path):\n",
        "    if not os.path.exists(path):\n",
        "        return {}\n",
        "    content = open(path, 'r', encoding='utf-8').read().strip()\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        out = {}\n",
        "        for line in content.splitlines():\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                if isinstance(obj, dict):\n",
        "                    out.update(obj)\n",
        "            except:\n",
        "                continue\n",
        "        return out\n",
        "\n",
        "def merge_activity_features(paths):\n",
        "    combined = {}\n",
        "    for p in paths:\n",
        "        data = load_json_file(p)\n",
        "        for k, feat in data.items():\n",
        "            vals = feat if isinstance(feat, (list, tuple)) else [feat]\n",
        "            vals = [safe_float(x) for x in vals]\n",
        "            if k in combined:\n",
        "                combined[k] = [(a + b) / 2 for a, b in zip(combined[k], vals)]\n",
        "            else:\n",
        "                combined[k] = vals\n",
        "    return combined\n",
        "\n",
        "def build_feature_matrix(json_data, id_map, feat_dim=32):\n",
        "    mat = torch.zeros(len(id_map), feat_dim)\n",
        "    for key, idx in id_map.items():\n",
        "        vec = torch.tensor([safe_float(x) for x in json_data.get(key, [])], dtype=torch.float32)\n",
        "        vec = torch.cat([vec, torch.zeros(max(0, feat_dim - vec.numel()))])[:feat_dim]\n",
        "        mat[idx] = vec\n",
        "    return mat\n",
        "\n",
        "def get_batch_side_info(seqs, feat_matrix):\n",
        "    B, L = seqs.shape\n",
        "    out = []\n",
        "    for i in range(B):\n",
        "        mask = seqs[i] != 0\n",
        "        if mask.sum() > 0:\n",
        "            out.append(feat_matrix[seqs[i][mask]].mean(0))\n",
        "        else:\n",
        "            out.append(torch.zeros(feat_matrix.size(1), device=feat_matrix.device))\n",
        "    return torch.stack(out, 0)\n",
        "\n",
        "# === Dataset & Time2Vec ===\n",
        "class Time2Vec(nn.Module):\n",
        "    def __init__(self, in_f, out_f):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_f, 1)\n",
        "        self.per = nn.Linear(in_f, out_f - 1)\n",
        "    def forward(self, t):\n",
        "        return torch.cat([self.lin(t), torch.sin(self.per(t))], -1)\n",
        "\n",
        "class MOOCubeCourseDataset(Dataset):\n",
        "    def __init__(self, csv_path, min_seq_len=2, max_seq_len=50, min_interactions=5, split=\"train\"):\n",
        "        df = pd.read_csv(csv_path, header=None, names=[\"user\",\"item\",\"time\"]).sort_values([\"user\",\"time\"])\n",
        "        df[\"dt\"] = df.groupby(\"user\")[\"time\"].diff().fillna(0)\n",
        "        df[\"dt\"] /= df[\"dt\"].max() or 1\n",
        "        self.u2i, self.i2i = {}, {}\n",
        "        df[\"u_idx\"] = df.user.map(lambda x: self.u2i.setdefault(x, len(self.u2i)))\n",
        "        df[\"i_idx\"] = df.item.map(lambda x: self.i2i.setdefault(x, len(self.i2i)))\n",
        "        grp = df.groupby(\"u_idx\")[['i_idx','dt']].apply(lambda g: list(zip(g.i_idx, g.dt)))\n",
        "        grp = grp[grp.map(len) >= min_interactions]\n",
        "        self.data = []\n",
        "        for seq in grp:\n",
        "            L = len(seq); tcut, vcut = int(0.8*L), int(0.9*L)\n",
        "            if split==\"train\" and 2 <= tcut < L:\n",
        "                inp, tgt = seq[:tcut], seq[tcut][0]\n",
        "            elif split==\"val\" and 2 <= vcut < L:\n",
        "                inp, tgt = seq[:vcut], seq[vcut][0]\n",
        "            elif split==\"test\" and L-1 >= 2:\n",
        "                inp, tgt = seq[:-1], seq[-1][0]\n",
        "            else:\n",
        "                continue\n",
        "            if len(inp) > max_seq_len:\n",
        "                inp = inp[-max_seq_len:]\n",
        "            self.data.append((\n",
        "                torch.tensor([i for i,_ in inp], dtype=torch.long),\n",
        "                torch.tensor([d for _,d in inp], dtype=torch.float32),\n",
        "                torch.tensor(tgt, dtype=torch.long)\n",
        "            ))\n",
        "        print(f\"[{split}] {len(self.data)} sequences\")\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "class EnhancedMOOCDataset(MOOCubeCourseDataset):\n",
        "    def __init__(self, *args, neg_samples=5, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.neg_samples = neg_samples\n",
        "        pop = defaultdict(int)\n",
        "        for seq,_,t in self.data:\n",
        "            for i in seq: pop[i.item()] += 1\n",
        "        top = sorted(pop.items(), key=lambda x:-x[1])[:2000]\n",
        "        self.pop_items   = [i for i,_ in top]\n",
        "        counts = torch.tensor([c for _,c in top], dtype=torch.float)\n",
        "        self.pop_weights = counts / counts.sum()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, dts, tgt = super().__getitem__(idx)\n",
        "        w = self.pop_weights.clone()\n",
        "        if tgt.item() in self.pop_items:\n",
        "            w[self.pop_items.index(tgt.item())] = 0\n",
        "        w = w / w.sum() if w.sum()>0 else w\n",
        "        negs = torch.multinomial(w, self.neg_samples, replacement=False)\n",
        "        negs = torch.tensor(self.pop_items, dtype=torch.long)[negs]\n",
        "        return seq, dts, tgt, negs\n",
        "\n",
        "def collate_fn(batch):\n",
        "    M = max(len(x[0]) for x in batch)\n",
        "    out = []\n",
        "    for it in batch:\n",
        "        s,d,t = it[0], it[1], it[2]\n",
        "        pad = M - len(s)\n",
        "        s = torch.cat([s, torch.zeros(pad, dtype=torch.long)])\n",
        "        d = torch.cat([d, torch.zeros(pad)])\n",
        "        if len(it)==4:\n",
        "            out.append((s,d,t,it[3]))\n",
        "        else:\n",
        "            out.append((s,d,t))\n",
        "    return tuple(torch.stack(cols) for cols in zip(*out))\n",
        "\n",
        "# === Side-Info Processors & Disentanglement ===\n",
        "class EnhancedSideInfoProcessor(nn.Module):\n",
        "    def __init__(self, dims, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.ModuleDict({\n",
        "            n: nn.Sequential(\n",
        "                nn.Linear(dim, d_model*2), nn.GELU(),\n",
        "                nn.Linear(d_model*2, d_model), nn.LayerNorm(d_model)\n",
        "            ) for n, dim in dims.items()\n",
        "        })\n",
        "        self.attn = nn.MultiheadAttention(d_model, 8, batch_first=True)\n",
        "        self.ffn  = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model*4), nn.GELU(),\n",
        "            nn.Linear(d_model*4, d_model), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.ln   = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        P = [self.proj[n](f) for n, f in feats.items()]\n",
        "        F = torch.stack(P, 1)\n",
        "        A, _ = self.attn(F, F, F)\n",
        "        H = self.ln(A + F)\n",
        "        return self.ffn(H) + H\n",
        "\n",
        "class CrossModalDisentanglement(nn.Module):\n",
        "    def __init__(self, dims, d_model):\n",
        "        super().__init__()\n",
        "        self.priv = nn.ModuleDict({\n",
        "            n: nn.Sequential(nn.Linear(dim, d_model), nn.ReLU(), nn.LayerNorm(d_model))\n",
        "            for n, dim in dims.items()\n",
        "        })\n",
        "        total = len(dims) * d_model\n",
        "        self.shared = nn.Sequential(nn.Linear(total, d_model), nn.ReLU(), nn.LayerNorm(d_model))\n",
        "\n",
        "    def forward(self, feats):\n",
        "        privates = [self.priv[n](f) for n, f in feats.items()]\n",
        "        cat      = torch.cat(privates, -1)\n",
        "        sh       = self.shared(cat)\n",
        "        return sh, privates\n",
        "\n",
        "def orthogonality_loss(privates):\n",
        "    if len(privates) < 2:\n",
        "        return 0.0\n",
        "    loss, n = 0.0, len(privates)\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            loss += (privates[i] * privates[j]).sum(-1).mean()\n",
        "    return loss / (n*(n-1)/2)\n",
        "\n",
        "# === Model & Loss ===\n",
        "class EnhancedFusionRecommenderWithDisentanglement(nn.Module):\n",
        "    def __init__(self, num_items, d_model=768, side_info_dims={}, max_seq_len=50,\n",
        "                 lambda_aux=0.5, mask_rate=0.15, ablation=None):\n",
        "        super().__init__()\n",
        "        self.ablation    = ablation\n",
        "        self.lambda_aux  = lambda_aux\n",
        "        self.mask_rate   = 0.0 if ablation==\"no_masked\" else mask_rate\n",
        "        self.emb_item = nn.Embedding(num_items, d_model, padding_idx=0)\n",
        "        self.emb_pos  = nn.Embedding(max_seq_len, d_model)\n",
        "        self.ortho_weight = 0.0 if ablation==\"no_orthogonality\" else 0.1\n",
        "        if ablation == \"no_time\":\n",
        "            self.emb_time = lambda t: torch.zeros(t.size(0), t.size(1), d_model, device=t.device)\n",
        "        else:\n",
        "            self.emb_time = Time2Vec(1, d_model)\n",
        "        self.side_proc = EnhancedSideInfoProcessor(side_info_dims, d_model)\n",
        "        if ablation == \"no_disentanglement\":\n",
        "            self.disent = None\n",
        "        else:\n",
        "            self.disent = CrossModalDisentanglement(side_info_dims, d_model)\n",
        "        self.side_attn    = nn.MultiheadAttention(d_model, 2, batch_first=True)\n",
        "        cfg = BertConfig(\n",
        "            hidden_size=d_model, num_hidden_layers=8,\n",
        "            num_attention_heads=8, intermediate_size=d_model*4,\n",
        "            hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1\n",
        "        )\n",
        "        self.transformer  = BertModel(cfg)\n",
        "        self.proj_head    = nn.Sequential(nn.Linear(d_model,d_model), nn.GELU(), nn.Linear(d_model,d_model))\n",
        "        self.pred_head    = nn.Linear(d_model, num_items)\n",
        "        self.mask_head    = nn.Linear(d_model, num_items)\n",
        "        self.tau          = nn.Parameter(torch.tensor(0.07))\n",
        "\n",
        "    def forward(self, seqs, dts, side_info=None):\n",
        "        B, L = seqs.shape\n",
        "        pos = torch.arange(L, device=seqs.device).unsqueeze(0)\n",
        "        x   = self.emb_item(seqs) + self.emb_pos(pos) + self.emb_time(dts.unsqueeze(-1))\n",
        "        pri = None\n",
        "        if side_info is not None and self.ablation != \"no_sideinfo\":\n",
        "            base = self.side_proc(side_info).mean(dim=1)\n",
        "            if self.disent is not None:\n",
        "                shared, pri = self.disent(side_info)\n",
        "            else:\n",
        "                shared, pri = base, None\n",
        "            st = torch.stack([base, shared], 1)\n",
        "            f, _ = self.side_attn(st, st, st)\n",
        "            x = x + f.mean(dim=1).unsqueeze(1)\n",
        "        out  = self.transformer(inputs_embeds=x).last_hidden_state\n",
        "        main = out[:,0,:]\n",
        "        proj = nn.functional.normalize(self.proj_head(main), dim=-1)\n",
        "        logits = self.pred_head(main)\n",
        "        ml = None\n",
        "        if self.training and self.mask_rate > 0:\n",
        "            m = (torch.rand(B, L, device=seqs.device) < self.mask_rate)\n",
        "            m[:,0] = False\n",
        "            if m.any():\n",
        "                part = out[m]\n",
        "                ml   = nn.functional.cross_entropy(self.mask_head(part), seqs[m])\n",
        "        return logits, proj, self.tau, pri, ml\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.3, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.alpha        = alpha\n",
        "        self.ls           = label_smoothing\n",
        "        self.ortho_weight = 0.1\n",
        "    def ce(self, logits, tgt):\n",
        "        n  = logits.size(-1)\n",
        "        oh = torch.zeros_like(logits).scatter(1, tgt.unsqueeze(1), 1)\n",
        "        oh = oh*(1-self.ls) + self.ls/n\n",
        "        lp = nn.functional.log_softmax(logits, dim=-1)\n",
        "        return -(oh*lp).sum(-1).mean()\n",
        "    def contrast(self, pos, negs, tau):\n",
        "        tau = tau.clamp(min=0.01)\n",
        "        sim = torch.cosine_similarity(pos.unsqueeze(1), negs, dim=-1)\n",
        "        return -torch.log(torch.exp(sim/tau).mean(-1)+1e-8).mean()\n",
        "    def forward(self, logits, tgt, pos, negs, tau, ortho=None):\n",
        "        l1 = self.ce(logits, tgt)\n",
        "        l2 = self.contrast(pos, negs, tau)\n",
        "        L  = (1-self.alpha)*l1 + self.alpha*l2\n",
        "        if ortho is not None:\n",
        "            L += self.ortho_weight * ortho\n",
        "        return L\n",
        "\n",
        "visual_cfg = BertConfig(\n",
        "    hidden_size=768, num_hidden_layers=8, num_attention_heads=8,\n",
        "    intermediate_size=768*4, hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1, output_attentions=True\n",
        ")\n",
        "\n",
        "class EnhancedFusionRecommenderWithVisuals(EnhancedFusionRecommenderWithDisentanglement):\n",
        "    def __init__(self, *a, **k):\n",
        "        super().__init__(*a, **k)\n",
        "        self.transformer = BertModel(visual_cfg)\n",
        "    def forward(self, seqs, dts, side_info=None):\n",
        "        logits, proj, tau, pri, ml = super().forward(seqs, dts, side_info)\n",
        "        B, L = seqs.shape\n",
        "        pos    = torch.arange(L, device=seqs.device).unsqueeze(0)\n",
        "        x      = self.emb_item(seqs) + self.emb_pos(pos) + self.emb_time(dts.unsqueeze(-1))\n",
        "        if side_info is not None:\n",
        "            base, _ = self.disent(side_info)\n",
        "            shared, _ = base, None\n",
        "            st        = torch.stack([base, shared],1)\n",
        "            f, _      = self.side_attn(st, st, st)\n",
        "            x         = x + f.mean(dim=1).unsqueeze(1)\n",
        "        out = self.transformer(inputs_embeds=x)\n",
        "        return logits, proj, tau, pri, ml, out.attentions\n",
        "\n",
        "# === Training, Evaluation & Ablation ===\n",
        "def enhanced_evaluate(model, loader, device, use_side_info=False, side_info_matrices=None, zero_time=False):\n",
        "    model.eval()\n",
        "    hr_list, ndcg_list, p_list, mrr_list = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            seqs, dts, tgt = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            if zero_time:\n",
        "                dts = torch.zeros_like(dts)\n",
        "            if use_side_info:\n",
        "                si = {n: get_batch_side_info(seqs, mtx) for n, mtx in side_info_matrices.items()}\n",
        "                logits, *_ = model(seqs, dts, side_info=si)\n",
        "            else:\n",
        "                logits, *_ = model(seqs, dts)\n",
        "            topk = torch.topk(logits, 5, dim=1)[1]\n",
        "            bs    = seqs.size(0)\n",
        "            hr_b=ndcg_b=p_b=mrr_b=0.0\n",
        "            for i in range(bs):\n",
        "                t0 = tgt[i].item()\n",
        "                preds = topk[i].tolist()\n",
        "                if t0 in preds:\n",
        "                    r = preds.index(t0)+1\n",
        "                    hr_b   += 1\n",
        "                    ndcg_b += 1/math.log2(r+1)\n",
        "                    p_b    += 1.0/5\n",
        "                    mrr_b  += 1.0/r\n",
        "            hr_list.append(hr_b/bs)\n",
        "            ndcg_list.append(ndcg_b/bs)\n",
        "            p_list.append(p_b/bs)\n",
        "            mrr_list.append(mrr_b/bs)\n",
        "    def ms(x): return (np.mean(x), np.std(x, ddof=1) if len(x)>1 else 0.0)\n",
        "    hr_m,hr_s      = ms(hr_list)\n",
        "    ndcg_m,ndcg_s  = ms(ndcg_list)\n",
        "    p_m,p_s        = ms(p_list)\n",
        "    mrr_m,mrr_s    = ms(mrr_list)\n",
        "    return {\n",
        "        'hr':hr_m, 'hr_std':hr_s,\n",
        "        'ndcg':ndcg_m, 'ndcg_std':ndcg_s,\n",
        "        'precision':p_m, 'precision_std':p_s,\n",
        "        'mrr':mrr_m, 'mrr_std':mrr_s\n",
        "    }\n",
        "\n",
        "def run_training(hp, train, val, si_mats, dev, num_items, si_dims, use_si=True, mask_rate_override=None, lambda_aux_override=None, ortho_weight=0.1, zero_time=False):\n",
        "    mask_rate   = mask_rate_override if mask_rate_override is not None else hp['mask_rate']\n",
        "    lambda_aux  = lambda_aux_override if lambda_aux_override is not None else hp['lambda_aux']\n",
        "    model = EnhancedFusionRecommenderWithDisentanglement(\n",
        "        num_items=num_items, side_info_dims=si_dims,\n",
        "        lambda_aux=lambda_aux, mask_rate=mask_rate\n",
        "    ).to(dev)\n",
        "    opt    = AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['weight_decay'])\n",
        "    total  = len(train) * hp['num_epochs']\n",
        "    sch    = get_cosine_schedule_with_warmup(opt, int(0.1*total), total)\n",
        "    crit   = CombinedLoss(alpha=0.3, label_smoothing=0.1)\n",
        "    scaler = GradScaler()\n",
        "    best, no_imp = 0, 0\n",
        "\n",
        "    for _ in range(hp['num_epochs']):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        for batch in train:\n",
        "            seqs, dts, tgt = batch[0].to(dev), batch[1].to(dev), batch[2].to(dev)\n",
        "            if zero_time:\n",
        "                dts = torch.zeros_like(dts)\n",
        "            negs = batch[3].to(dev) if len(batch)==4 else None\n",
        "            si   = {n:get_batch_side_info(seqs, m) for n,m in si_mats.items()} if use_si else None\n",
        "\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                logits, pos, tau, priv, ml = model(seqs, dts, side_info=si)\n",
        "                if negs is not None:\n",
        "                    with torch.no_grad():\n",
        "                        neg_emb = nn.functional.normalize(model.emb_item(negs), dim=-1)\n",
        "                    loss = (1-crit.alpha)*crit.ce(logits, tgt) + crit.alpha*crit.contrast(pos, neg_emb, tau)\n",
        "                else:\n",
        "                    loss = crit.ce(logits, tgt)\n",
        "                if priv and ortho_weight>0:\n",
        "                    loss += ortho_weight * orthogonality_loss(priv)\n",
        "                if ml is not None:\n",
        "                    loss += lambda_aux * ml\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sch.step()\n",
        "\n",
        "        val_m = enhanced_evaluate(model, val, dev, use_side_info=use_si, side_info_matrices=si_mats, zero_time=zero_time)\n",
        "        if val_m['ndcg'] > best:\n",
        "            best, no_imp = val_m['ndcg'], 0\n",
        "        else:\n",
        "            no_imp += 1\n",
        "        if no_imp >= hp['patience']:\n",
        "            break\n",
        "\n",
        "    return best, model.state_dict()\n",
        "\n",
        "def run_training_ablation(hp, train, val, si_mats, dev, num_items, si_dims, ablation):\n",
        "    model = EnhancedFusionRecommenderWithDisentanglement(\n",
        "        num_items=num_items,\n",
        "        side_info_dims=si_dims,\n",
        "        lambda_aux=hp['lambda_aux'],\n",
        "        mask_rate=hp['mask_rate'],\n",
        "        ablation=ablation\n",
        "    ).to(dev)\n",
        "    opt    = AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['weight_decay'])\n",
        "    total  = len(train) * hp['num_epochs']\n",
        "    sch    = get_cosine_schedule_with_warmup(opt, int(0.1*total), total)\n",
        "    crit   = CombinedLoss(alpha=0.3, label_smoothing=0.1)\n",
        "    scaler = GradScaler()\n",
        "    best, no_imp = 0, 0\n",
        "\n",
        "    for _ in range(hp['num_epochs']):\n",
        "        model.train()\n",
        "        for batch in train:\n",
        "            seqs, dts, tgt = batch[0].to(dev), batch[1].to(dev), batch[2].to(dev)\n",
        "            negs = batch[3].to(dev) if len(batch)==4 else None\n",
        "            si   = {n:get_batch_side_info(seqs, m) for n,m in si_mats.items()} if (ablation!=\"no_sideinfo\") else None\n",
        "\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                logits, pos, tau, priv, ml = model(seqs, dts, side_info=si)\n",
        "                loss = (1-crit.alpha)*crit.ce(logits, tgt) + crit.alpha*crit.contrast(pos, (nn.functional.normalize(model.emb_item(negs), dim=-1) if negs is not None else pos), tau)\n",
        "                if priv is not None:\n",
        "                    loss += model.ortho_weight * orthogonality_loss(priv)\n",
        "                if ml is not None:\n",
        "                    loss += model.lambda_aux * ml\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sch.step()\n",
        "\n",
        "        val_m = enhanced_evaluate(\n",
        "            model, val, dev,\n",
        "            use_side_info=(ablation!=\"no_sideinfo\"),\n",
        "            side_info_matrices=si_mats,\n",
        "            zero_time=(ablation==\"no_time\")\n",
        "        )\n",
        "        if val_m['ndcg'] > best:\n",
        "            best, no_imp = val_m['ndcg'], 0\n",
        "        else:\n",
        "            no_imp += 1\n",
        "        if no_imp >= hp['patience']:\n",
        "            break\n",
        "\n",
        "    return best, model.state_dict()\n",
        "\n",
        "# === Part 5/5: main_all with 10 seeds & bootstrap CIs ===\n",
        "def main_all():\n",
        "    start_time = time.time()\n",
        "    csv = \"MOOCCube/MOOCQA/tuples/rel_user_course.csv\"\n",
        "\n",
        "    # Data pipelines\n",
        "    tr_ds = EnhancedMOOCDataset(csv, split=\"train\", neg_samples=5)\n",
        "    vl_ds = MOOCubeCourseDataset(csv, split=\"val\")\n",
        "    te_ds = MOOCubeCourseDataset(csv, split=\"test\")\n",
        "    tr_ld = DataLoader(tr_ds, batch_size=32, shuffle=True,  collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
        "    vl_ld = DataLoader(vl_ds, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
        "    te_ld = DataLoader(te_ds, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
        "    dev   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Side-info\n",
        "    course_j  = load_json_file(\"MOOCCube/entities/course.json\")\n",
        "    teacher_j = load_json_file(\"MOOCCube/entities/teacher.json\")\n",
        "    act_j     = merge_activity_features([\n",
        "        \"MOOCCube/additional_information/prerequisite_prediction.json\",\n",
        "        \"MOOCCube/additional_information/concept_infomation.json\",\n",
        "        \"MOOCCube/additional_information/user_video_act.json\"\n",
        "    ])\n",
        "    c_m_cpu = build_feature_matrix(course_j, tr_ds.i2i)\n",
        "    t_m_cpu = build_feature_matrix(teacher_j, tr_ds.i2i)\n",
        "    a_m_cpu = build_feature_matrix(act_j, tr_ds.i2i)\n",
        "    si_mats_cpu = {'course':c_m_cpu, 'teacher':t_m_cpu, 'activity':a_m_cpu}\n",
        "    si_mats     = {k: m.to(dev) for k,m in si_mats_cpu.items()}\n",
        "    num_items = len(tr_ds.i2i)\n",
        "    si_dims   = {'course':32,'teacher':32,'activity':32}\n",
        "\n",
        "    # Hyperparam search\n",
        "    hparam_grid = [\n",
        "        {'lr':1e-5,'weight_decay':0.01,'num_epochs':80,'patience':10,'lambda_aux':0.1,'mask_rate':0.1},\n",
        "        {'lr':1e-5,'weight_decay':0.001,'num_epochs':80,'patience':10,'lambda_aux':0.1,'mask_rate':0.1},\n",
        "        {'lr':3e-5,'weight_decay':0.01,'num_epochs':80,'patience':10,'lambda_aux':0.1,'mask_rate':0.1},\n",
        "        {'lr':3e-5,'weight_decay':0.001,'num_epochs':80,'patience':10,'lambda_aux':0.1,'mask_rate':0.1},\n",
        "    ]\n",
        "    best_ndcg, best_hp, best_state = 0, None, None\n",
        "    for hp in hparam_grid:\n",
        "        ndcg, state = run_training(hp, tr_ld, vl_ld, si_mats, dev, num_items, si_dims, use_si=True)\n",
        "        if ndcg > best_ndcg:\n",
        "            best_ndcg, best_hp, best_state = ndcg, hp, state\n",
        "\n",
        "    # Final 10-seed evaluation\n",
        "    seeds = [42, 123, 999, 2025, 31415, 271828, 314159, 161803, 27182, 141421]\n",
        "    all_results = []\n",
        "    for seed in seeds:\n",
        "        set_seed(seed)\n",
        "        _, state = run_training(best_hp, tr_ld, vl_ld, si_mats, dev, num_items, si_dims, use_si=True)\n",
        "        model = EnhancedFusionRecommenderWithDisentanglement(\n",
        "            num_items=num_items, side_info_dims=si_dims,\n",
        "            lambda_aux=best_hp['lambda_aux'], mask_rate=best_hp['mask_rate']\n",
        "        ).to(dev)\n",
        "        model.load_state_dict(state)\n",
        "        m = enhanced_evaluate(model, te_ld, dev, use_side_info=True, side_info_matrices=si_mats)\n",
        "        all_results.append(m)\n",
        "\n",
        "    # Bootstrap 95% CIs\n",
        "    hr_lower, hr_upper, hr_mean       = bootstrap_ci([r['hr']   for r in all_results])\n",
        "    ndcg_lower, ndcg_upper, ndcg_mean = bootstrap_ci([r['ndcg'] for r in all_results])\n",
        "    p_lower, p_upper, p_mean          = bootstrap_ci([r['precision'] for r in all_results])\n",
        "    mrr_lower, mrr_upper, mrr_mean    = bootstrap_ci([r['mrr'] for r in all_results])\n",
        "\n",
        "    print(f\"Final (over seeds {seeds}) → \"\n",
        "          f\"HR@5:   {hr_mean:.4f} [{hr_lower:.4f}, {hr_upper:.4f}], \"\n",
        "          f\"NDCG@5: {ndcg_mean:.4f} [{ndcg_lower:.4f}, {ndcg_upper:.4f}], \"\n",
        "          f\"P@5:    {p_mean:.4f} [{p_lower:.4f}, {p_upper:.4f}], \"\n",
        "          f\"MRR@5:  {mrr_mean:.4f} [{mrr_lower:.4f}, {mrr_upper:.4f}]\")\n",
        "\n",
        "    # Ablation studies\n",
        "    print(\"=== Ablation studies ===\")\n",
        "    for ab in [\"no_sideinfo\",\"no_masked\",\"no_disentanglement\",\"no_time\",\"no_orthogonality\"]:\n",
        "        _, st = run_training_ablation(best_hp, tr_ld, vl_ld, si_mats, dev, num_items, si_dims, ab)\n",
        "        model = EnhancedFusionRecommenderWithDisentanglement(\n",
        "            num_items=num_items, side_info_dims=si_dims,\n",
        "            lambda_aux=best_hp['lambda_aux'], mask_rate=best_hp['mask_rate']\n",
        "        ).to(dev)\n",
        "        model.load_state_dict(st, strict=False)\n",
        "        m = enhanced_evaluate(model, te_ld, dev,\n",
        "                              use_side_info=(ab!=\"no_sideinfo\"),\n",
        "                              side_info_matrices=si_mats,\n",
        "                              zero_time=(ab==\"no_time\"))\n",
        "        print(f\"Ablation {ab:<15} → \"\n",
        "              f\"HR@5: {m['hr']:.4f}±{m['hr_std']:.4f}, \"\n",
        "              f\"NDCG@5: {m['ndcg']:.4f}±{m['ndcg_std']:.4f}, \"\n",
        "              f\"P@5: {m['precision']:.4f}±{m['precision_std']:.4f}, \"\n",
        "              f\"MRR@5: {m['mrr']:.4f}±{m['mrr_std']:.4f}\")\n",
        "\n",
        "    # Visualizations & SHAP/IG\n",
        "    entropy_records = []\n",
        "    torch.cuda.empty_cache()\n",
        "    vis_model = EnhancedFusionRecommenderWithVisuals(\n",
        "        num_items=num_items,\n",
        "        side_info_dims=si_dims,\n",
        "        lambda_aux=best_hp['lambda_aux'],\n",
        "        mask_rate=best_hp['mask_rate']\n",
        "    ).cpu()\n",
        "    vis_model.load_state_dict(best_state, strict=False)\n",
        "    vl_vis = DataLoader(vl_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "    seqs, dts, tgt     = next(iter(vl_vis))\n",
        "    side_cpu           = {k: get_batch_side_info(seqs, m) for k,m in si_mats_cpu.items()}\n",
        "    logits, proj, tau, priv, ml, attn_weights = vis_model(seqs, dts, side_info=side_cpu)\n",
        "\n",
        "    num_layers = len(attn_weights)\n",
        "    for layer in (0, num_layers//2, num_layers-1):\n",
        "        ents, hmax_idx, hmin_idx = compute_layer_head_entropies(attn_weights, layer)\n",
        "        for head_i, ent in enumerate(ents):\n",
        "            entropy_records.append({'layer': layer+1, 'head': head_i+1, 'entropy': ent})\n",
        "        plot_layer_entropy(attn_weights, layer)\n",
        "        print(f\"Layer {layer+1}: max ent head={hmax_idx+1}, min ent head={hmin_idx+1}\")\n",
        "        plot_attention_heatmap(attn_weights, layer, hmax_idx, title=\"(max ent)\")\n",
        "        plot_attention_heatmap(attn_weights, layer, hmin_idx, title=\"(min ent)\")\n",
        "\n",
        "    pd.DataFrame(entropy_records).to_excel(\"entropy_values.xlsx\", index=False)\n",
        "    print(\"Entropy values saved to entropy_values.xlsx\")\n",
        "\n",
        "    # SHAP/IG example case\n",
        "    try:\n",
        "        print(\"\\n=== SHAP/IG örnek vaka incelemesi ===\")\n",
        "        seqs, dts, tgt   = next(iter(vl_vis))\n",
        "        seq              = seqs[0:1].to(dev)\n",
        "        dts_seq          = dts[0:1].to(dev)\n",
        "        mask_pos         = seq.size(1) - 1\n",
        "        side_one         = {k: v[0:1].to(dev) for k,v in side_cpu.items()}\n",
        "\n",
        "        ig_model = EnhancedFusionRecommenderWithVisuals(\n",
        "            num_items=num_items,\n",
        "            side_info_dims=si_dims,\n",
        "            lambda_aux=best_hp['lambda_aux'],\n",
        "            mask_rate=best_hp['mask_rate']\n",
        "        ).to(dev)\n",
        "        ig_model.load_state_dict(best_state)\n",
        "        ig_model.eval()\n",
        "        for p in ig_model.parameters(): p.requires_grad_(False)\n",
        "\n",
        "        item_emb  = ig_model.emb_item(seq)\n",
        "        pos_idx   = torch.arange(seq.size(1), device=dev).unsqueeze(0)\n",
        "        pos_emb   = ig_model.emb_pos(pos_idx)\n",
        "        time_emb  = ig_model.emb_time(dts_seq.unsqueeze(-1))\n",
        "        embed_in  = (item_emb + pos_emb + time_emb).detach().requires_grad_(True)\n",
        "\n",
        "        def emb_forward(emb, side=side_one):\n",
        "            x         = emb\n",
        "            base, _   = ig_model.side_proc(side).mean(1), None\n",
        "            shared, _ = ig_model.disent(side)\n",
        "            fuse, _   = ig_model.side_attn(\n",
        "                torch.stack([base, shared],1),\n",
        "                torch.stack([base, shared],1),\n",
        "                torch.stack([base, shared],1)\n",
        "            )\n",
        "            x         = x + fuse.mean(1).unsqueeze(1)\n",
        "            out       = ig_model.transformer(inputs_embeds=x).last_hidden_state[:,0,:]\n",
        "            logits    = ig_model.pred_head(out)\n",
        "            return logits[:, mask_pos]\n",
        "\n",
        "        ig = IntegratedGradients(emb_forward)\n",
        "        baseline = torch.zeros_like(embed_in)\n",
        "        atts, delta = ig.attribute(\n",
        "            inputs=embed_in,\n",
        "            baselines=baseline,\n",
        "            return_convergence_delta=True,\n",
        "            n_steps=50\n",
        "        )\n",
        "        ig_scores = atts[0].abs().sum(-1).detach().cpu().numpy()\n",
        "        contrib   = ig_scores[mask_pos]\n",
        "        print(f\"Mask position contribution to MRR (IG): +{contrib:.4f}\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.bar(range(len(ig_scores)), ig_scores)\n",
        "        plt.xlabel(\"Sequence Position\")\n",
        "        plt.ylabel(\"Total IG Contribution\")\n",
        "        plt.title(\"Position-wise IG Contributions (Embedding-based)\")\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"SHAP/IG example error:\", e)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"🏁 Total runtime: {(end_time - start_time)/60:.2f} min\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_all()\n"
      ]
    }
  ]
}